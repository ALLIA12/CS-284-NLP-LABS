{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "mUswXp0-tlNV",
    "ExecuteTime": {
     "end_time": "2025-03-14T14:54:21.035882Z",
     "start_time": "2025-03-14T14:54:21.031915Z"
    }
   },
   "source": [
    "from IPython.display import clear_output, Markdown, display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DMHK_Wpytpr3",
    "ExecuteTime": {
     "end_time": "2025-03-14T14:54:22.368756Z",
     "start_time": "2025-03-14T14:54:22.366732Z"
    }
   },
   "source": [
    "# No need to run this on colab. These libraries come pre-installed on colab\n",
    "# %pip install torch torchvision torchaudio"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16xxZX0Ft4XL"
   },
   "source": [
    "# Content:\n",
    "\n",
    "In this demo, we will take a look at the llama-v2 language model.\n",
    "\n",
    "To use llama-v2, we will use the llama-cpp-python library, which is a python converted version of the llama-cpp library\n",
    "\n",
    "For this, we need to install the library and to download the model weights file. The file can be downloaded from huggingface [repo](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) of [TheBloke](https://huggingface.co/TheBloke). Credits to him for quantizing the model, saving it in different formats like GGML and GGUF and sharing with the community. He has a lot of other models on his channel that you can check out, including different versions of llama (like the 70B param sized one and code llama etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mnqKpa4vvrk"
   },
   "source": [
    "## Downloading model file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UejyX_MntcGt"
   },
   "source": [
    "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n",
    "\n",
    "clear_output()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvuFeDu3wZDd"
   },
   "source": [
    "## Installing llama-cpp-python\n",
    "\n",
    "installing supports different versions of hardware acceleration.\n",
    "\n",
    "We will go with Cuda. Checkout the [Github Repo](https://github.com/abetlen/llama-cpp-python) for more options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317219,
     "status": "ok",
     "timestamp": 1712190421119,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "qG7Mz0UuwCty",
    "outputId": "edd30f04-03de-4dfa-cacf-585c4c4ae62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.59.tar.gz (37.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m37.4/37.4 MB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Installing backend dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.5/45.5 kB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.59-cp310-cp310-manylinux_2_35_x86_64.whl size=38831179 sha256=1100ff5c2ff4803156a8073d02da4c102667cdd57aa36b92c59ebfaa2b17dd98\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/d1/01/78c9a09d3b1f348adbc9dad22b238dba3e20c12479986b8efa\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.59\n"
     ]
    }
   ],
   "source": "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.74  # This takes a few mins when building wheel. Be patient."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0F2JhjiyiiD"
   },
   "source": [
    "## Running Llama-v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caVzdA9vxri4",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:03:07.061502Z",
     "start_time": "2025-03-14T15:03:06.891468Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "from llama_cpp import Llama"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GTiK6FuFyqk6",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:03:44.161828Z",
     "start_time": "2025-03-14T15:03:13.110419Z"
    }
   },
   "source": [
    "model = Llama(\n",
    "    \"llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=-1, # To use GPU\n",
    "    n_ctx=2048, # default ctx window is 512\n",
    "\n",
    "    # llama-cpp supports multiple variations of llama and they can have different prompt format.\n",
    "    # specifying the chat format isn't necessary but will let us use the create_chat_completions function.\n",
    "    # alternatively we can just restructure our prompt and use the simple __call__\n",
    "    chat_format=\"llama-2\",\n",
    ")\n",
    "\n",
    "# clear_output()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j60Xyt0b49KW"
   },
   "source": [
    "Let's try a code generation example.\n",
    "\n",
    "Note that llama-v2 and llama-v2 chat isn't specifically trained for code generations but it has some capability for it\n",
    "\n",
    "Models trained for code generation are what's used by applications like github copilot or codenium.\n",
    "\n",
    "Llama-2, being a text generation model can be trained and used for that purpose as well.\n",
    "There is even a version of llama-2 called codellama, build for code generation.\n",
    "\n",
    "The model file for codellama can also be found at TheBloke's repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZM1D8ZG1ql8a",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:04:00.153640Z",
     "start_time": "2025-03-14T15:04:00.151273Z"
    }
   },
   "source": [
    "sys_msg = \"You are a Python coding instructor. Help your students with their code.\"\n",
    "user_prompt = \"\"\"\n",
    "Write me a function to calculate the fibbonaci series uptil length N.\n",
    "Do NOT use recursion. Instead, use dynamic programming.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{sys_msg}\n",
    "<</SYS>>\n",
    "{user_prompt}[/INST]\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13210,
     "status": "ok",
     "timestamp": 1712180580978,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "KCUObzRh1wtc",
    "outputId": "c31bcf2e-55ab-497a-a0d4-e6838ef7fb17",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:04:10.685477Z",
     "start_time": "2025-03-14T15:04:02.897543Z"
    }
   },
   "source": [
    "output = model(\n",
    "    prompt,\n",
    "    max_tokens=None  # sets no length limit\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     795.34 ms\n",
      "llama_print_timings:      sample time =      74.72 ms /   502 runs   (    0.15 ms per token,  6718.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     795.27 ms /    68 tokens (   11.70 ms per token,    85.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6237.09 ms /   501 runs   (   12.45 ms per token,    80.33 tokens per second)\n",
      "llama_print_timings:       total time =    7781.36 ms /   569 tokens\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1712180610536,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "ty1NLAjk10pr",
    "outputId": "222f44d5-affc-4c87-cea4-a86e7dac8d91",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:04:15.407046Z",
     "start_time": "2025-03-14T15:04:15.402920Z"
    }
   },
   "source": [
    "output"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-cdd3c728-c118-43b6-b18d-d82650dcda44',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1741964642,\n",
       " 'model': 'llama-2-7b-chat.Q5_K_M.gguf',\n",
       " 'choices': [{'text': \"Great! I'd be happy to help you with that. Here's a function in Python to calculate the Fibonacci series up to length N without using recursion:\\n```python\\ndef fib_dynamic(N):\\n    # Create a 2D array to store the Fibonacci series\\n    fib = [[0] * (N+1), [0]]\\n    # Initialize the first two terms of the series\\n    fib[0][0] = fib[1][0] = 1\\n    # Calculate the next term in the series\\n    for i in range(2, N+1):\\n        fib[i][0] = fib[i-1][0] + fib[i-1][1]\\n    return fib\\n```\\nHere's how the function works:\\n1. We create a 2D array `fib` with two rows and `N+1` columns. The first row contains the initial values of 1 and 1, respectively. The second row is empty for now.\\n2. We initialize the first term of the series by setting `fib[0][0] = fib[1][0] = 1`.\\n3. Now comes the key part of the function - calculating the next term in the series. We use a loop that ranges from 2 to `N+1` to calculate each term. For each `i`, we use the previous terms' values to calculate the current term. Specifically, we add the previous term's value to the previous term's second value (i.e., `fib[i-1][1]`). This is the formula for the Fibonacci sequence: `F(n) = F(n-1) + F(n-2)`.\\n4. After the loop, we return the `fib` array to get the calculated values of the Fibonacci series up to length `N`.\\nHere's an example usage of the function:\\n```python\\n# Calculate the Fibonacci series up to length 5\\nfib = fib_dynamic(5)\\nprint(fib)\\n# Output: [[0, 1, 1, 2, 3], [0]]\\n```\\nI hope this helps! Let me know if you have any questions or need further clarification.\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 68, 'completion_tokens': 501, 'total_tokens': 569}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1712180612628,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "YwZ68PIA2HHr",
    "outputId": "2d9ae434-54e8-4b5d-8973-7b9aa1a1d50a",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:04:21.905800Z",
     "start_time": "2025-03-14T15:04:21.902419Z"
    }
   },
   "source": [
    "output[\"choices\"][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Great! I'd be happy to help you with that. Here's a function in Python to calculate the Fibonacci series up to length N without using recursion:\\n```python\\ndef fib_dynamic(N):\\n    # Create a 2D array to store the Fibonacci series\\n    fib = [[0] * (N+1), [0]]\\n    # Initialize the first two terms of the series\\n    fib[0][0] = fib[1][0] = 1\\n    # Calculate the next term in the series\\n    for i in range(2, N+1):\\n        fib[i][0] = fib[i-1][0] + fib[i-1][1]\\n    return fib\\n```\\nHere's how the function works:\\n1. We create a 2D array `fib` with two rows and `N+1` columns. The first row contains the initial values of 1 and 1, respectively. The second row is empty for now.\\n2. We initialize the first term of the series by setting `fib[0][0] = fib[1][0] = 1`.\\n3. Now comes the key part of the function - calculating the next term in the series. We use a loop that ranges from 2 to `N+1` to calculate each term. For each `i`, we use the previous terms' values to calculate the current term. Specifically, we add the previous term's value to the previous term's second value (i.e., `fib[i-1][1]`). This is the formula for the Fibonacci sequence: `F(n) = F(n-1) + F(n-2)`.\\n4. After the loop, we return the `fib` array to get the calculated values of the Fibonacci series up to length `N`.\\nHere's an example usage of the function:\\n```python\\n# Calculate the Fibonacci series up to length 5\\nfib = fib_dynamic(5)\\nprint(fib)\\n# Output: [[0, 1, 1, 2, 3], [0]]\\n```\\nI hope this helps! Let me know if you have any questions or need further clarification.\",\n",
       " 'index': 0,\n",
       " 'logprobs': None,\n",
       " 'finish_reason': 'stop'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1712180614028,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "6SstiIZl14K4",
    "outputId": "c9accd0b-d9bd-4f96-a30e-0d04ca74d2e6",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:04:25.869643Z",
     "start_time": "2025-03-14T15:04:25.866420Z"
    }
   },
   "source": [
    "# response\n",
    "display(Markdown(output[\"choices\"][0][\"text\"]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Great! I'd be happy to help you with that. Here's a function in Python to calculate the Fibonacci series up to length N without using recursion:\n```python\ndef fib_dynamic(N):\n    # Create a 2D array to store the Fibonacci series\n    fib = [[0] * (N+1), [0]]\n    # Initialize the first two terms of the series\n    fib[0][0] = fib[1][0] = 1\n    # Calculate the next term in the series\n    for i in range(2, N+1):\n        fib[i][0] = fib[i-1][0] + fib[i-1][1]\n    return fib\n```\nHere's how the function works:\n1. We create a 2D array `fib` with two rows and `N+1` columns. The first row contains the initial values of 1 and 1, respectively. The second row is empty for now.\n2. We initialize the first term of the series by setting `fib[0][0] = fib[1][0] = 1`.\n3. Now comes the key part of the function - calculating the next term in the series. We use a loop that ranges from 2 to `N+1` to calculate each term. For each `i`, we use the previous terms' values to calculate the current term. Specifically, we add the previous term's value to the previous term's second value (i.e., `fib[i-1][1]`). This is the formula for the Fibonacci sequence: `F(n) = F(n-1) + F(n-2)`.\n4. After the loop, we return the `fib` array to get the calculated values of the Fibonacci series up to length `N`.\nHere's an example usage of the function:\n```python\n# Calculate the Fibonacci series up to length 5\nfib = fib_dynamic(5)\nprint(fib)\n# Output: [[0, 1, 1, 2, 3], [0]]\n```\nI hope this helps! Let me know if you have any questions or need further clarification."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-tQxdpO69wo"
   },
   "source": [
    "Let's try a translation example now\n",
    "\n",
    "\n",
    "\n",
    "Like before, llama-v2 has some capability of translating but is not SPECIFICALLY trained for it\n",
    "\n",
    "The translations here will vary in quality depending on what languages are being used.\n",
    "\n",
    "The more of that language llama-v2 has seen during training, the better the results\n",
    "\n",
    "For example, the translation of english to japanese will be better than from english to arabic.\n",
    "\n",
    "instead of using llama-2-chat, it'd be better to use a finetuned seq2seq model for translations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rda-Lcfa6z2H",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:05:11.964436Z",
     "start_time": "2025-03-14T15:05:11.961394Z"
    }
   },
   "source": [
    "# Let's use create_chat_completions for now.\n",
    "# Note that we need to specify the chat format for this to work correctly\n",
    "\n",
    "\n",
    "sys_msg = \"You are state of the are languge translator who is assisting people with translations\"\n",
    "user_prompt = \"\"\"\n",
    "Translate this English to Japanese:\n",
    "'I woke up really early in the morning and then went for a jog to freshen up my mind'\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6233,
     "status": "ok",
     "timestamp": 1712184356737,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "FipmD9rb74j_",
    "outputId": "30bc8503-b507-4e45-deee-ce0c61760df3",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:05:16.432817Z",
     "start_time": "2025-03-14T15:05:13.030102Z"
    }
   },
   "source": [
    "output = model.create_chat_completion(\n",
    "    messages = [\n",
    "          {\"role\": \"system\", \"content\": sys_msg},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": user_prompt\n",
    "          }\n",
    "      ]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     795.34 ms\n",
      "llama_print_timings:      sample time =      36.24 ms /   235 runs   (    0.15 ms per token,  6484.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.21 ms /    72 tokens (    2.21 ms per token,   452.25 tokens per second)\n",
      "llama_print_timings:        eval time =    2912.69 ms /   234 runs   (   12.45 ms per token,    80.34 tokens per second)\n",
      "llama_print_timings:       total time =    3396.83 ms /   306 tokens\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1712184358308,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "Mi_KBqJI8PSj",
    "outputId": "016bf668-3424-4172-9670-fa47f605bf89",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:05:32.278273Z",
     "start_time": "2025-03-14T15:05:32.274756Z"
    }
   },
   "source": [
    "output"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-60d861f2-3525-4a73-89d6-9d976fd9b180',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1741964713,\n",
       " 'model': 'llama-2-7b-chat.Q5_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '  Sure, I\\'d be happy to help! Here\\'s the translation of \"I woke up really early in the morning and then went for a jog to freshen up my mind\" into Japanese:\\n「朝に起きたのは非常に早く、そして脳を清算するためにランニングをしました」\\nHere\\'s a breakdown of the translation:\\n* 朝 (asobu) - morning\\n* に (ni) - at/in\\n* 起きた (okiru) - woke up\\n* 非常に (hikari ni) - really early\\n* 早く (hayaku) - early\\n* そして (soshite) - and then\\n* 脳を清算する (kokoro o seisaneru) - to freshen up my mind\\n* ランニング (running) - jogging\\nI hope this helps! Let me know if you have any other questions.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 73, 'completion_tokens': 234, 'total_tokens': 307}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1712184359911,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "vC5iU1uv8UNG",
    "outputId": "23e88f77-7630-456d-88f3-c08d84dfed1e",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:05:35.802666Z",
     "start_time": "2025-03-14T15:05:35.800127Z"
    }
   },
   "source": [
    "print(output['choices'][0]['message']['content'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to help! Here's the translation of \"I woke up really early in the morning and then went for a jog to freshen up my mind\" into Japanese:\n",
      "「朝に起きたのは非常に早く、そして脳を清算するためにランニングをしました」\n",
      "Here's a breakdown of the translation:\n",
      "* 朝 (asobu) - morning\n",
      "* に (ni) - at/in\n",
      "* 起きた (okiru) - woke up\n",
      "* 非常に (hikari ni) - really early\n",
      "* 早く (hayaku) - early\n",
      "* そして (soshite) - and then\n",
      "* 脳を清算する (kokoro o seisaneru) - to freshen up my mind\n",
      "* ランニング (running) - jogging\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6whTF6sHMyn"
   },
   "source": [
    "For the next example, let's ask it to solve a simple mathematical equation\n",
    "\n",
    "for this example, we will also restrict it to produce JSON output which is more practical when the output needs to be further processed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oOSLfCLU88-x",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:06:38.977740Z",
     "start_time": "2025-03-14T15:06:38.974299Z"
    }
   },
   "source": [
    "sys_msg = \"\"\"\n",
    "You will be given reviews and your job is to analyse them, find and extract the negative parts.\n",
    "\n",
    "1. Only output negative parts. If a review has no negative experiences in it, just output [].\n",
    "2. One review can have multiple negative parts. find them all.\n",
    "3. Do not alter parts of reviews or add something that isn't in the review.\n",
    "4. Use the same keyword for the same issue across reviews. Do not use multiple keywords for the same issue.\n",
    "5. Make sure the output is a valid JSON\n",
    "\n",
    "Use this JSON format and dont say ANYTHING other than this JSON:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"keyword-for-the-problem\": \"Part of the review mentioning the problem\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Analyse this review: The food was decent, but nothing extraordinary. It felt a bit overpriced for what we got. Service was a bit slow, but the staff was polite.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{sys_msg}\n",
    "<</SYS>>\n",
    "{user_prompt}[/INST]\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1712192961344,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "CjEljkDSInW3",
    "outputId": "664c68b3-9221-461c-e429-79877ee7c039",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:06:44.397958Z",
     "start_time": "2025-03-14T15:06:44.054044Z"
    }
   },
   "source": [
    "output = model(\n",
    "    prompt,\n",
    "    max_tokens=None  # sets no length limit\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     795.34 ms\n",
      "llama_print_timings:      sample time =       3.09 ms /    20 runs   (    0.15 ms per token,  6466.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =      77.15 ms /   231 tokens (    0.33 ms per token,  2994.21 tokens per second)\n",
      "llama_print_timings:        eval time =     234.64 ms /    19 runs   (   12.35 ms per token,    80.98 tokens per second)\n",
      "llama_print_timings:       total time =     337.67 ms /   250 tokens\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JGdcWA70IrF2",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:06:48.227886Z",
     "start_time": "2025-03-14T15:06:48.225093Z"
    }
   },
   "source": [
    "output = json.loads(output[\"choices\"][0][\"text\"])"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712192965106,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "yaABPAkmJ4YJ",
    "outputId": "536b5653-0a3c-48b2-909a-3b4d7b7c6b89",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:06:50.303960Z",
     "start_time": "2025-03-14T15:06:50.300935Z"
    }
   },
   "source": [
    "output"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword-for-the-problem': 'overpriced'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm4WEyLnJarN"
   },
   "source": [
    "As most likely will be the case when you re-run this, the output has some issues.\n",
    "\n",
    "Let's try **one shotting** by adding an example in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2094,
     "status": "ok",
     "timestamp": 1712193426963,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "JAQUP5vnItCi",
    "outputId": "44c25a5a-d2e4-4308-e2e6-7a52f273d911",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:07:06.492708Z",
     "start_time": "2025-03-14T15:07:05.905427Z"
    }
   },
   "source": [
    "sys_msg = \"\"\"\n",
    "You will be given reviews and your job is to analyse them, find and extract the negative parts.\n",
    "\n",
    "1. Only output negative parts. If a review has no negative experiences in it, just output [].\n",
    "2. One review can have multiple negative parts. find them all.\n",
    "3. Do not alter parts of reviews or add something that isn't in the review.\n",
    "4. Use the same keyword for the same issue across reviews. Do not use multiple keywords for the same issue.\n",
    "5. Make sure the output is a valid JSON\n",
    "\n",
    "Use this JSON format and dont say ANYHING other than this JSON:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"keyword-for-the-problem\": \"Part of the review mentioning the problem\"\n",
    "    }\n",
    "]\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "Review: `Decent food, but nothing outstanding. The service was average, and the atmosphere was a bit lacking. It's an okay option if you're in the area.`\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"boring-ambience\": \"atmosphere was a bit lacking\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "\n",
    "Review: `The food was decent, but nothing extraordinary. It felt a bit overpriced for what we got. Service was a bit slow, but the staff was polite.`\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[INST]<<SYS>>\n",
    "{sys_msg}\n",
    "<</SYS>>\n",
    "{user_prompt}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "output = model(\n",
    "    prompt,\n",
    "    max_tokens=None,  # sets no length limit\n",
    "    stop=['<END>']\n",
    ")\n",
    "\n",
    "output = json.loads(output[\"choices\"][0][\"text\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     795.34 ms\n",
      "llama_print_timings:      sample time =       3.49 ms /    24 runs   (    0.15 ms per token,  6868.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.97 ms /   305 tokens (    0.87 ms per token,  1146.75 tokens per second)\n",
      "llama_print_timings:        eval time =     284.08 ms /    23 runs   (   12.35 ms per token,    80.96 tokens per second)\n",
      "llama_print_timings:       total time =     580.07 ms /   328 tokens\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712193428839,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "lpWu516NrNtl",
    "outputId": "268462df-59a4-45fe-b047-a8e4c82d8f2c",
    "ExecuteTime": {
     "end_time": "2025-03-14T15:07:07.939585Z",
     "start_time": "2025-03-14T15:07:07.935984Z"
    }
   },
   "source": [
    "output"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overpriced': 'It felt a bit overpriced for what we got'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKpEaQHHMN5B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
