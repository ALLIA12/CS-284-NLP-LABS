{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face Transformers\n",
    "\n",
    "This notebook provides an introduction to the Hugging Face `transformers` library and demonstrates several key applications of transformer models.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Introduction to Hugging Face and Transformers\n",
    "2. Setting up the environment\n",
    "3. Text Classification with BERT\n",
    "4. Named Entity Recognition\n",
    "5. Text Generation with GPT-2\n",
    "6. Question Answering with a Transformer\n",
    "7. Text Summarization\n",
    "8. Fine-tuning a Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Hugging Face and Transformers\n",
    "\n",
    "Hugging Face is an AI company that has developed the `transformers` library, which provides thousands of pre-trained models to perform tasks on text, images, and audio. The library is built on top of PyTorch and TensorFlow and has become the industry standard for deploying transformer models.\n",
    "\n",
    "Transformers are a type of neural network architecture introduced in the paper \"Attention Is All You Need\" (Vaswani et al., 2017). They revolutionized NLP by enabling parallel processing of sequences and capturing long-range dependencies through self-attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "Let's start by installing the necessary libraries and importing the required modules."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:21.140002Z",
     "start_time": "2025-03-13T14:18:21.137662Z"
    }
   },
   "source": [
    "# Install the transformers library\n",
    "#!pip install transformers datasets torch evaluate"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:23.864296Z",
     "start_time": "2025-03-13T14:18:21.141055Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 17:18:22.966758: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 17:18:22.976191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741875502.986497   25919 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741875502.989292   25919 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 17:18:23.000224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classification with BERT\n",
    "\n",
    "One of the most common applications of transformer models is text classification. We'll use BERT (Bidirectional Encoder Representations from Transformers) to classify text sentiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:24.965398Z",
     "start_time": "2025-03-13T14:18:23.865573Z"
    }
   },
   "source": [
    "# Load the sentiment analysis pipeline with DistilBERT (a lighter version of BERT)\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Define some example texts\n",
    "texts = [\n",
    "    \"I absolutely loved this movie! The acting was fantastic.\",\n",
    "    \"The service at this restaurant was terrible, and the food was cold.\",\n",
    "    \"The product works as expected, nothing special.\",\n",
    "    \"I'm not sure if I'll recommend this book to others.\"\n",
    "]\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:25.069623Z",
     "start_time": "2025-03-13T14:18:24.966522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze the sentiment of each text\n",
    "results = sentiment_classifier(texts)\n",
    "\n",
    "# Display the results\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I absolutely loved this movie! The acting was fantastic.\n",
      "Sentiment: POSITIVE, Score: 0.9999\n",
      "\n",
      "Text: The service at this restaurant was terrible, and the food was cold.\n",
      "Sentiment: NEGATIVE, Score: 0.9997\n",
      "\n",
      "Text: The product works as expected, nothing special.\n",
      "Sentiment: NEGATIVE, Score: 0.9988\n",
      "\n",
      "Text: I'm not sure if I'll recommend this book to others.\n",
      "Sentiment: NEGATIVE, Score: 0.9441\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Text Classification\n",
    "\n",
    "Let's see how to use a specific BERT model for a custom classification task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:25.533768Z",
     "start_time": "2025-03-13T14:18:25.070785Z"
    }
   },
   "source": [
    "# Load a pre-trained BERT model and tokenizer for sequence classification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:25.646508Z",
     "start_time": "2025-03-13T14:18:25.534664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom example\n",
    "text = \"The new smartphone has excellent features, but the price is too high.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "print(inputs)\n",
    "\n",
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Print results\n",
    "positive_score = predictions[0, 1].item()\n",
    "negative_score = predictions[0, 0].item()\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Positive score: {positive_score:.4f}\")\n",
    "print(f\"Negative score: {negative_score:.4f}\")\n",
    "print(f\"Predicted sentiment: {'Positive' if positive_score > negative_score else 'Negative'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  2047, 26381,  2038,  6581,  2838,  1010,  2021,  1996,\n",
      "          3976,  2003,  2205,  2152,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Text: The new smartphone has excellent features, but the price is too high.\n",
      "Positive score: 0.0089\n",
      "Negative score: 0.9911\n",
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) identifies entities such as persons, organizations, locations, etc., in text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:26.849651Z",
     "start_time": "2025-03-13T14:18:25.648214Z"
    }
   },
   "source": [
    "# Load a more recent NER model\n",
    "model_name = \"dslim/bert-base-NER\"  # This model is known to work well with named entities\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline with the loaded model\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Example text for NER\n",
    "text = \"KAUST Academy is planning on creating a new program sponsored by the Ministry of Environment, Water and Agriculture, There is an Ali here\"\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:26.861157Z",
     "start_time": "2025-03-13T14:18:26.850702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run NER\n",
    "results = ner(text)\n",
    "\n",
    "# Print results in a more readable format\n",
    "for entity in results:\n",
    "    print(f\"Entity: {entity['word']}\")\n",
    "    print(f\"Type: {entity['entity_group']}\")\n",
    "    print(f\"Confidence: {entity['score']:.4f}\")\n",
    "    print(\"-\" * 30)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: KAUST Academy\n",
      "Type: ORG\n",
      "Confidence: 0.9959\n",
      "------------------------------\n",
      "Entity: Ministry of Environment, Water and Agriculture\n",
      "Type: ORG\n",
      "Confidence: 0.9980\n",
      "------------------------------\n",
      "Entity: Ali\n",
      "Type: PER\n",
      "Confidence: 0.4834\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing NER Results\n",
    "\n",
    "Let's create a visualization for our NER results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:27.194376Z",
     "start_time": "2025-03-13T14:18:26.862046Z"
    }
   },
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Install spacy if needed\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "text = \"KAUST Academy is planning on creating a new program sponsored by the Ministry of Environment, Water and Agriculture, There is an Ali here\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "# Convert to format for displaCy visualization\n",
    "doc_entities = {\"text\": text, \"ents\": [], \"title\": None}\n",
    "for entity in entities:\n",
    "    #print(entity)\n",
    "    doc_entities[\"ents\"].append(\n",
    "        {\"start\": entity[\"start\"], \"end\": entity[\"end\"], \"label\": entity[\"entity_group\"]}\n",
    "    )\n",
    "\n",
    "# Display the visualization\n",
    "displacy.render(doc_entities, style=\"ent\", manual=True, jupyter=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    KAUST Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is planning on creating a new program sponsored by the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ministry of Environment, Water and Agriculture\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", There is an \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ali\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " here</div></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation with GPT-2\n",
    "\n",
    "Transformer models can generate coherent and contextually relevant text. Let's use GPT-2 for text generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:27.979687Z",
     "start_time": "2025-03-13T14:18:27.195323Z"
    }
   },
   "source": [
    "# Load text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text using different prompts\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of transportation involves\",\n",
    "    \"Climate change will impact\",\n",
    "    \"KAUST Is\"\n",
    "]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:50.918980Z",
     "start_time": "2025-03-13T14:18:27.980610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for prompt in prompts:\n",
    "    # Generate text\n",
    "    generated_text = generator(\n",
    "        prompt, \n",
    "        max_length=1000,\n",
    "        truncation=True,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  \n",
    "        top_p=0.9,        # Nucleus sampling parameter\n",
    "        do_sample=True,    # Use sampling instead of greedy decoding\n",
    "        \n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text[0]['generated_text']}\\n\")\n",
    "    print(\"-\"*50)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Artificial intelligence is\n",
      "Generated: Artificial intelligence is a new field that has been attracting a lot of attention in recent years, but it's still only a few years old. A lot of the work that is done is done in a few dozen different domains, so it's not clear if it's feasible to get all that work done in one place, or whether that's a good thing or a bad thing.\n",
      "\n",
      "So, we decided to start with one of our own. We had an idea for a solution that we thought would be a great solution for a lot of people, and that's what we're doing.\n",
      "\n",
      "One of the biggest challenges we faced was figuring out how to make it work. We started by writing a bunch of tests, and then we had to figure out how to write them. That took a lot of time. We spent a lot of time figuring out what the test should do, how to implement it, and then we had to figure out how to do it on the fly.\n",
      "\n",
      "We had to figure out how to make it work.\n",
      "\n",
      "We had to figure out how to make it work.\n",
      "\n",
      "We had to figure out how to make it work.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "We had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "We had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "And then we had to figure out how to do it on the fly.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of transportation involves\n",
      "Generated: The future of transportation involves a more equitable distribution of economic resources, which means that public transportation should be considered a means to achieve that goal.\n",
      "\n",
      "This is an important point, and one that has been made repeatedly by other states. The state of Indiana is well placed to make that case.\n",
      "\n",
      "In 2012, Indiana became the first state in the nation to require its citizens to carry a concealed carry permit. As a result, the state has become the first state in the nation to require its citizens to carry a concealed carry permit.\n",
      "\n",
      "The Indiana Legislature passed a law in November that requires all Indiana residents to carry a concealed carry permit, which will allow law enforcement to carry concealed weapons in public. The law was passed by a vote of 11 to 4.\n",
      "\n",
      "It is important to note that this law was not just a measure to make Indiana law-abiding citizens safer, but also to ensure that law enforcement officers and the public will be able to carry concealed weapons safely.\n",
      "\n",
      "The law, which was signed into law by Gov. Mike Pence, was signed into law by Gov. Mike Pence. It is a law that allows law enforcement officers to carry concealed weapons in public.\n",
      "\n",
      "The law also allows law enforcement officers to carry concealed weapons in public on a daily basis. It also allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "There is a number of different types of concealed carry permits that are available.\n",
      "\n",
      "There are also state-issued, concealed carry permits that are available to law enforcement officers.\n",
      "\n",
      "There are also state-issued, concealed carry permits that are available to law enforcement officers.\n",
      "\n",
      "There are also state-issued, concealed carry permits that are available to law enforcement officers.\n",
      "\n",
      "There are also state-issued, concealed carry permits that are available to law enforcement officers.\n",
      "\n",
      "There are also state-issued, concealed carry permits that are available to law enforcement officers.\n",
      "\n",
      "State-issued, concealed carry permits that are available to law enforcement officers are not required by law.\n",
      "\n",
      "A state-issued concealed carry permit is a firearm that is not a prohibited weapon.\n",
      "\n",
      "A state-issued concealed carry permit is a firearm that is not a prohibited weapon.\n",
      "\n",
      "A state-issued concealed carry permit is a firearm that is not a prohibited weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered a dangerous weapon.\n",
      "\n",
      "The law allows law enforcement officers to carry concealed weapons in public in a manner that is not considered\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Climate change will impact\n",
      "Generated: Climate change will impact on your life. If you live in a climate-controlled area, you should always be aware of the effects of climate change.\n",
      "\n",
      "There are many factors that can affect your life, including the weather, climate, and weather patterns. The following are some of the most important factors that affect your life:\n",
      "\n",
      "What kind of food you eat\n",
      "\n",
      "What type of shelter you have\n",
      "\n",
      "What kind of equipment you use\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of clothes you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What kind of food you have\n",
      "\n",
      "What type of clothing you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home you have\n",
      "\n",
      "What type of health care you have\n",
      "\n",
      "What type of food you have\n",
      "\n",
      "What type of equipment you have\n",
      "\n",
      "What type of home\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt: KAUST Is\n",
      "Generated: KAUST Isolation\n",
      "\n",
      "Possible to use a high-power LED light bulb to light the whole house.\n",
      "\n",
      "This is a good idea if you are considering a DIY project. The LED light is located under the refrigerator and is attached to the wall with a wire. A large LED light bulb is used to brighten the room.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house. The idea is to have the whole house light up as well as the entire kitchen. The idea is to have the whole house light up as well as the whole kitchen.\n",
      "\n",
      "If you are looking to have a large LED bulb, you should be able to use it to make a large room or a small room. If you are looking to have a large LED bulb, you should be able to use it to make a large room or a small room.\n",
      "\n",
      "A low power LED light bulb is used to light the whole house.\n",
      "\n",
      "This is the main part of the project. It is used to light the whole house. This is the main part of the project. It is used to light the whole house.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house. The idea is to have the whole house light up as well as the entire kitchen. The idea is to have the whole house light up as well as the entire kitchen.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house. The idea is to have the whole house light up as well as the entire kitchen. The idea is to have the whole house light up as well as the entire kitchen.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room. If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house. The idea is to have the whole house light up as well as the entire kitchen.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room. If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "The idea is to have the whole house light up as well as the entire kitchen. The bulbs are placed under a large metal plate. This is used to light the whole house. The idea is to have the whole house light up as well as the entire kitchen. The idea is to have the whole house light up as well as the entire kitchen.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room. If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb, you should be able to use it to make a small room or a small room.\n",
      "\n",
      "If you are looking to have a low-power LED light bulb\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Text Generation Parameters\n",
    "\n",
    "Let's explore how different parameters affect text generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:55.037040Z",
     "start_time": "2025-03-13T14:18:50.920868Z"
    }
   },
   "source": [
    "# Load a larger GPT-2 model for better generation\n",
    "generator_large = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
    "\n",
    "# Single prompt with different temperature settings\n",
    "prompt = \"The key to successful machine learning is\"\n",
    "temperatures = [0.2, 0.7, 1.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated_text = generator_large(\n",
    "        prompt, \n",
    "        max_length=100, \n",
    "        num_return_sequences=1,\n",
    "        temperature=temp,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"Generated: {generated_text[0]['generated_text']}\\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2\n",
      "Generated: The key to successful machine learning is to be able to predict what will happen in the future. This is why machine learning is so important for the future of the Internet.\n",
      "\n",
      "The key to successful machine learning is to be able to predict what will happen in the future. This is why machine learning is so important for the future of the Internet.\n",
      "\n",
      "The key to successful machine learning is to be able to predict what will happen in the future. This is why machine learning is so important for\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7\n",
      "Generated: The key to successful machine learning is to be aware of the various types of problems that can arise. We will then discuss the different categories of problems that can be solved and the tools that can be used to achieve them.\n",
      "\n",
      "\n",
      "How Machine Learning Works\n",
      "\n",
      "An example of a problem that can be solved with machine learning is predicting the likelihood of an event occurring. Given the following data sets:\n",
      "\n",
      "X-Files episode 5.5.6\n",
      "\n",
      "X-Files episode 5.5.\n",
      "\n",
      "Temperature: 1.2\n",
      "Generated: The key to successful machine learning is understanding the trade-offs between algorithms and human cognition. If you treat software as a commodity rather than the engine of cognition, there is a tendency to ignore or ignore those limitations, to put too much effort into solving difficult problems in ways human intuition can't. This reduces computing to algorithmic analysis, and to reduce the computational complexity of machine learning, making deep learning very much like neural nets or traditional deep learning pipelines in computing science. The only way to make deep\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Question Answering with a Transformer\n",
    "\n",
    "Transformer models can be used for extractive question answering, where the answer is extracted from a given context."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:55.510821Z",
     "start_time": "2025-03-13T14:18:55.038185Z"
    }
   },
   "source": [
    "# Load question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Define context and questions\n",
    "context = \"\"\"\n",
    "Transformers are neural network architectures that have revolutionized machine learning, particularly in the field of Natural Language Processing (NLP). \n",
    "They were introduced in a 2017 paper titled 'Attention Is All You Need' by researchers at Google. The key innovation was the self-attention mechanism, \n",
    "which allows the model to weigh the importance of different words in a sentence when making predictions. This addressed limitations in previous \n",
    "sequence-to-sequence models that used recurrent neural networks (RNNs). Popular transformer models include BERT (developed by Google), GPT (developed by OpenAI(Sadly, and I hate them)), \n",
    "and T5 (also by Google). These models have been pre-trained on massive text datasets and can be fine-tuned for specific tasks.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:55.541076Z",
     "start_time": "2025-03-13T14:18:55.511857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"Who developed BERT?\",\n",
    "    \"What is the key innovation in transformers?\",\n",
    "    \"When were transformers introduced?\",\n",
    "    \"What limitations did transformers address?\",\n",
    "    \"Do i like openai ?\"\n",
    "]\n",
    "\n",
    "# Get answers for each question\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer['answer']}\")\n",
    "    print(f\"Confidence: {answer['score']:.4f}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who developed BERT?\n",
      "Answer: Google\n",
      "Confidence: 0.8852\n",
      "\n",
      "Question: What is the key innovation in transformers?\n",
      "Answer: self-attention mechanism\n",
      "Confidence: 0.6998\n",
      "\n",
      "Question: When were transformers introduced?\n",
      "Answer: 2017\n",
      "Confidence: 0.9215\n",
      "\n",
      "Question: What limitations did transformers address?\n",
      "Answer: sequence-to-sequence models\n",
      "Confidence: 0.2421\n",
      "\n",
      "Question: Do i like openai ?\n",
      "Answer: I hate them\n",
      "Confidence: 0.0294\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Summarization\n",
    "\n",
    "Transformers can generate concise summaries of longer texts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:56.758370Z",
     "start_time": "2025-03-13T14:18:55.542019Z"
    }
   },
   "source": [
    "# Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Text to summarize\n",
    "article = \"\"\"\n",
    "    \"Artificial intelligence (AI) has rapidly evolved in recent years, transforming various sectors including healthcare, finance, transportation, and entertainment. \\n\",\n",
    "    \"Machine learning algorithms, particularly deep learning models, have demonstrated remarkable capabilities in image recognition, natural language processing, \\n\",\n",
    "    \"and decision-making tasks. Companies worldwide are investing billions in AI research and development to gain competitive advantages.\\n\",\n",
    "    \"However, the rise of AI also brings significant challenges. Ethical concerns about bias in AI algorithms have emerged, as systems may perpetuate or amplify \\n\",\n",
    "    \"existing societal biases present in training data. Privacy issues are also prominent, especially with the vast amounts of personal data collected to train AI systems. \\n\",\n",
    "    \"Additionally, there are growing concerns about job displacement as automation capabilities increase.\\n\",\n",
    "    \"Regulatory frameworks are beginning to emerge globally to address these challenges. The European Union has proposed comprehensive AI regulations that categorize \\n\",\n",
    "    \"AI systems based on risk levels. The United States is developing sector-specific approaches, while China has implemented its own regulatory system focusing on \\n\",\n",
    "    \"algorithm transparency and data security.\\n\",\n",
    "    \"Despite these challenges, AI continues to advance rapidly. Researchers are working on more sophisticated models with enhanced reasoning capabilities, while also \\n\",\n",
    "    \"developing techniques to make AI more interpretable, fair, and aligned with human values. The future of AI will likely involve finding the right balance between \\n\",\n",
    "    \"technological innovation and ethical considerations.\\n\"\n",
    "\"\"\"\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:57.297249Z",
     "start_time": "2025-03-13T14:18:56.759429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate summary\n",
    "summary = summarizer(article, max_length=150, min_length=40, do_sample=False)\n",
    "\n",
    "print(\"Original text length:\", len(article))\n",
    "print(\"Summary length:\", len(summary[0]['summary_text']))\n",
    "print(\"\\nSummary:\")\n",
    "print(summary[0]['summary_text'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 1716\n",
      "Summary length: 507\n",
      "\n",
      "Summary:\n",
      " \"Artificial intelligence (AI) has rapidly evolved in recent years, transforming various sectors including healthcare, finance, transportation, and entertainment . Ethical concerns about bias in AI algorithms have emerged, as systems may perpetuate or amplify societal biases present in training data . Privacy issues are also prominent, especially with the vast amounts of personal data collected to train AI systems . There are growing concerns about job displacement as automation capabilities increase .\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning a Transformer Model\n",
    "\n",
    "While pre-trained models are powerful, fine-tuning them on specific datasets often improves performance for domain-specific tasks. Let's see how to fine-tune a BERT model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:18:57.300637Z",
     "start_time": "2025-03-13T14:18:57.298383Z"
    }
   },
   "source": [
    "# Install required libraries if not already installed\n",
    "#!pip install datasets evaluate transformers[torch]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:05.750753Z",
     "start_time": "2025-03-13T14:18:57.301690Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load a dataset (SST-2 for sentiment analysis)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:05.755712Z",
     "start_time": "2025-03-13T14:19:05.751836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(dataset)\n",
    "\n",
    "# Show a few examples\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "    if i < 5:  # Just show a few examples\n",
    "        print(f\"Text: {example['sentence']}\")\n",
    "        print(f\"Label: {example['label']} ({['negative', 'positive'][example['label']]})\\n\")\n",
    "    else:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "Text: hide new secretions from the parental units \n",
      "Label: 0 (negative)\n",
      "\n",
      "Text: contains no wit , only labored gags \n",
      "Label: 0 (negative)\n",
      "\n",
      "Text: that loves its characters and communicates something rather beautiful about human nature \n",
      "Label: 1 (positive)\n",
      "\n",
      "Text: remains utterly satisfied to remain the same throughout \n",
      "Label: 0 (negative)\n",
      "\n",
      "Text: on the worst revenge-of-the-nerds clichs the filmmakers could dredge up \n",
      "Label: 0 (negative)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:06.302825Z",
     "start_time": "2025-03-13T14:19:05.756868Z"
    }
   },
   "source": [
    "# Load pre-trained tokenizer and model\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare for training\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))  # Using a subset for demonstration\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali/.virtualenvs/PyTorchENV/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:07.367687Z",
     "start_time": "2025-03-13T14:19:06.303846Z"
    }
   },
   "source": [
    "# Define metric for evaluation\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:07.387454Z",
     "start_time": "2025-03-13T14:19:07.368641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define evaluation arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./pre_training_results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./pre_training_logs\",\n",
    "    report_to=\"none\", \n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:08.103132Z",
     "start_time": "2025-03-13T14:19:07.388851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Trainer for evaluation only\n",
    "pre_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model before fine-tuning\n",
    "print(\"Evaluating model before fine-tuning...\")\n",
    "pre_training_results = pre_trainer.evaluate(eval_dataset=eval_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model before fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:08.106835Z",
     "start_time": "2025-03-13T14:19:08.104252Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Pre-training accuracy: {pre_training_results['eval_accuracy']:.4f}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training accuracy: 0.4750\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:08.109926Z",
     "start_time": "2025-03-13T14:19:08.107767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Also test on some sample sentences\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie and would recommend it to anyone!\",\n",
    "    \"The plot was confusing and the acting was terrible.\",\n",
    "    \"It's neither great nor terrible, just average.\"\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:08.120995Z",
     "start_time": "2025-03-13T14:19:08.111001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize and predict\n",
    "test_encodings = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = {k: v.to(device) for k, v in test_encodings.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel predictions before fine-tuning:\")\n",
    "for sentence, prediction in zip(test_sentences, predictions):\n",
    "    positive_score = prediction[1].item()\n",
    "    sentiment = \"positive\" if positive_score > 0.5 else \"negative\"\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {sentiment} (positive score: {positive_score:.4f})\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model predictions before fine-tuning:\n",
      "Sentence: I really enjoyed this movie and would recommend it to anyone!\n",
      "Sentiment: negative (positive score: 0.4893)\n",
      "\n",
      "Sentence: The plot was confusing and the acting was terrible.\n",
      "Sentiment: negative (positive score: 0.4815)\n",
      "\n",
      "Sentence: It's neither great nor terrible, just average.\n",
      "Sentiment: negative (positive score: 0.4744)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:42.090887Z",
     "start_time": "2025-03-13T14:19:08.122119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.379482</td>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.464420</td>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.27543795518774206, metrics={'train_runtime': 33.7994, 'train_samples_per_second': 88.759, 'train_steps_per_second': 5.592, 'total_flos': 397402195968000.0, 'train_loss': 0.27543795518774206, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:42.759317Z",
     "start_time": "2025-03-13T14:19:42.091893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:42.768623Z",
     "start_time": "2025-03-13T14:19:42.760336Z"
    }
   },
   "source": [
    "\n",
    "print(f\"Evaluation accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Test on some new examples\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie and would recommend it to anyone!\",\n",
    "    \"The plot was confusing and the acting was terrible.\",\n",
    "    \"It's neither great nor terrible, just average.\"\n",
    "]\n",
    "\n",
    "# Tokenize and predict\n",
    "device = model.device\n",
    "test_encodings = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = {k: v.to(device) for k, v in test_encodings.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Print results\n",
    "for sentence, prediction in zip(test_sentences, predictions):\n",
    "    positive_score = prediction[1].item()\n",
    "    sentiment = \"positive\" if positive_score > 0.5 else \"negative\"\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {sentiment} (positive score: {positive_score:.4f})\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.8450\n",
      "Sentence: I really enjoyed this movie and would recommend it to anyone!\n",
      "Sentiment: positive (positive score: 0.9418)\n",
      "\n",
      "Sentence: The plot was confusing and the acting was terrible.\n",
      "Sentiment: negative (positive score: 0.1229)\n",
      "\n",
      "Sentence: It's neither great nor terrible, just average.\n",
      "Sentiment: negative (positive score: 0.3739)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the Hugging Face `transformers` library and demonstrated several key applications of transformer models:\n",
    "\n",
    "1. Text classification for sentiment analysis\n",
    "2. Named Entity Recognition (NER)\n",
    "3. Text generation with GPT-2\n",
    "4. Question answering\n",
    "5. Text summarization\n",
    "6. Fine-tuning a pre-trained model\n",
    "\n",
    "These applications demonstrate the versatility and power of transformer models in natural language processing tasks. The Hugging Face ecosystem makes it easy to leverage these models with minimal code, while also providing the flexibility to customize and fine-tune them for specific use cases.\n",
    "\n",
    "For more information and advanced use cases, refer to the official Hugging Face documentation: https://huggingface.co/docs/transformers/index\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:19:42.771145Z",
     "start_time": "2025-03-13T14:19:42.769572Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
